---
title: 'Graph Neural Networks'
date: 2023-12-01
permalink: 
tags:
  - Graphs
  - Graph Neural Networks
  - PyTorch Geometric
---

Have you ever heard about **graphs**? What about **graph neural networks (GNNs)**? 
If you have never paid attention, let's try to remind you that, structures like the
image below are, actually, _graphs_. And they are filling all the internet.

![](https://i0.wp.com/backgroundabstract.com/wp-content/uploads/edd/2022/01/gradient-network-connection-background_23-2148865392-e1656081168680.jpg?fit=626%2C417&ssl=1)
###### Figure 1: [A simple image background, that you can find anyplace in the internet.](https://i0.wp.com/backgroundabstract.com/wp-content/uploads/edd/2022/01/gradient-network-connection-background_23-2148865392-e1656081168680.jpg?fit=626%2C417&ssl=1)

Let's go with me, to learn a bit about this amazing technique, able to tackle problems 
beyond the usual _tabular data_ from the day-to-day to Cosmology!

This post can be followed in a Jupyter Notebook using my repo: [https://github.com/natalidesanti/pytorch_and_GNNs](https://github.com/natalidesanti/pytorch_and_GNNs).
If you prefer (to not have headaches with installation dependencies) you can follow this in `Google Colab`: [![Open this notebook in Colab].

Different data sets
--------------------------

Most of the time, we have a simple _tabular data_ and we would like to make predictions given some columns (the _input data_) and other
columns (the _target data_) of that table, which you would like to make some predictions using the input. 
This is the simplest idea behind "doing" traditional **machine learning**. 
What this kind of data has is that it is what we call a **structured data set**. 
This means that you will always have your _input_ with the same shape, because the object you are trying to use to learn predictions
has always the same number of features.

The problem arises when we have **unstructured data** and this kind of data is everywhere because everything can be translated into
a _graph_. In other words, this means you can have different _input data_ with different "number of objects" (what we will call, in
the graph structure, as **nodes**). And the beautiful thing here is that these different "pieces of data" have structure by themselves,
in the way that we define each node containing a fixed number of **node features**.
You can think about _unstructured data_ as a set of tables, each one with a different number of rows.
This is the kind of data **GNNs** are amazing to deal with!

![](https://raw.githubusercontent.com/natalidesanti/natalidesanti.github.io/master/images/structuresxunstructured_data.jpg)
###### Figure 2: A representation of structured X unstructured data sets.

What is a graph?
----------------------------

**Graphs** are mathematical units represented by $3$ numbers: $G = G (n_i, e_{i j}, g)$:
* $n_i$: node attributes (properties related to the objects translated into graphs)
* $e_{i j}$: edge attributes (usually properties related to the two nodes in question)
* $g$: global attribute (which represents the entire graph)

![](https://blog.fastforwardlabs.com/images/editor_uploads/2019-10-25-173715-graph_basics.png)
###### Figure 3: The idea of a graph - Image source: [https://blog.fastforwardlabs.com/images/editor_uploads/2019-10-25-173715-graph_basics.png](https://blog.fastforwardlabs.com/images/editor_uploads/2019-10-25-173715-graph_basics.png)

(Almost) everything can be translated into a **graph**!

Let's built the most simple graph ever: a graph with $3$ nodes: $[0, 1, 2]$, with node attributes $x_1 \in [-1, 0, 1]$ and $4$ edges (lines that take one node to another, as well theirs vice versa):

![](https://pytorch-geometric.readthedocs.io/en/latest/_images/graph.svg)
###### Figure 4: A simple graph, with all its information.

```
edge_index = torch.tensor([[0, 1], [1, 0], [1, 2], [2, 1]], dtype = torch.long) #Edge indexes
x = torch.tensor([[-1], [0], [1]], dtype = torch.float) #Node attributes
g = torch.tensor( [[3]], dtype = torch.int ) #Global attribute (number of nodes)
graph = Data(x = x, edge_index = edge_index.t().contiguous(), u = g)
```

To visualize it we can just use:

```
plt.figure(dpi = 100)
plt.title('Example of a graph colored by their node attribute')
G = to_networkx(graph, to_undirected = True)
nx.draw(G, node_color = graph.x[:, -1], node_size = 300, with_labels = True)
```

![](https://raw.githubusercontent.com/natalidesanti/natalidesanti.github.io/master/images/simple_visualization-graph.png)
###### Figure 5: An example of what you will see with the piece of code - a graph, colored by their node information

A simple problem: predicting the center of mass of a box
----------------------------

In order for me, to explain how GNNs work, I will let you think about a simple problem with me.
Imagine we need to compute the **center of mass** of some boxes, which contain some particles inside them.

![](https://studyingphysics.files.wordpress.com/2012/10/system-of-particles.jpg)
###### Figure 6: Center of mass definition - Image source: [https://studyingphysics.files.wordpress.com/2012/10/system-of-particles.jpg](https://studyingphysics.files.wordpress.com/2012/10/system-of-particles.jpg).

This means we will compute:
$\mathbf{r}_{CM} = \frac{ \sum_{i = 1}^{N} m_i \mathbf{r}_{i} }{\sum_{i = 1}^{N} m_i }$.

For simplicity we can consider only particles of the same mass, distributed uniformly into a box of side $L$, 
with always the same number of particles. Then, we can generate the dataset using the following function:

```
def box_generator(NB, NP, L):

    #NB : number of boxes
    #NP : number of particles
    #L : size of the box
    #r : position of the particles [0, 1]
    #dist : distance of the particle in question with respect to the center of mass
    #cm : center of mass of each box

    r = {}
    cm = np.zeros((NB, 3))

    for i in range(NB):

        r[i] = np.random.uniform(low = 0.0, high = L , size = (NP, 3))
        cm[i] = np.average(r[i], axis = 0)

    return r, cm
```

To simplificate our task, let's consider a box of side $L = 1.0$, $NP = 100$ particles per box (here, we could consider boxes
with different number of particles, and, then, we would end up with the best representation of unstructured dataset -- even thought,
this will be left as an exercise for you guys), and $NB = 2000$ boxes. These boxes will look like the figure bellow:

![](https://raw.githubusercontent.com/natalidesanti/natalidesanti.github.io/master/images/box_cm.png)
###### Figure 7: Visualization of one box, with particles and their center of mass.

Our task will be create a GNN to infer the $3$D value of the center of mass of each particle!

### Creating the graphs

Now, with 2000 boxes at hand, we have a huge task to translate each one of them into a graph.
That is why we will automatize this step. We can easily do this creating a routine which is responsible to read
each one of the catalogs and create a graph for it. Then, the sequence of graphs can be arranged into a list and
this will feed our GNN.

Creating the graph per se is a piece of art. Here I will show to you how to do this using `kNN` method. 
This means we will connect the particles in the box which are the $k$ neighbour of each other. This function will
give to us the edges, i.e, what particles will be connected to each other. For simplicity, let's choose $k = 10$. 
But, more than that, we have to choose the edge attributes. A simple choice is take their values as the modulus
of the distance between the two particles in question. Then, I wrote this into two different functions:

To compute the edges:

```
def get_edges(pos, k_number):
  '''
  Compute knn graph and get edges and edge features
  '''
  # 1. Get edges
  edge_index = knn_graph(torch.tensor(pos, dtype = torch.float32), k = k_number, loop = False)

  # 2. Get edge attributes
  row, col = edge_index
  diff = pos[row] - pos[col]

  # Distance
  dist = np.linalg.norm(diff, axis = 1)

  edge_attr = np.concatenate([dist.reshape(-1, 1)])

  return edge_index, edge_attr
```

To create the graphs, choosing as node features the particle positions

```
def create_graphs(pos, NB):

    #Getting the edge indexes and edge attributes
    edges = {}
    edges_atrs = {}
    for i in range(NB):
        edges[i], edges_atrs[i] = get_edges_kNN(pos[i], 10)

    #Getting the node attributes
    x = {}
    for i in range(NB):
        x[i] = np.zeros( ( pos[i].shape[0], 3 ) )
    for i in range(NB):
      x[i][:, 0] = pos[i][:, 0]
      x[i][:, 1] = pos[i][:, 1]
      x[i][:, 2] = pos[i][:, 2]

    #Getting the target attributes
    y = np.zeros( (NB, 3) )
    for i in range(NB):
      y[i, :] = (CM[i, :] - mean)/std #Data pre-processing the target value

    dataset = []
    for i in range(NB):
        edge_index = torch.tensor(edges[i].T, dtype = torch.long)
        edge_atrs = torch.tensor(edges_atrs[i], dtype = torch.float)
        xs = torch.tensor(x[i], dtype = torch.float32)
        ys = np.reshape(y[i], (1, 3))
        data = Data(x = xs, edge_index = edge_index.t().contiguous(), edge_atrs = edge_atrs, y = ys)
        dataset.append(data)

    return dataset
```

In the function above you can notice I am using `mean` and `std`, which correspond to a data pre-procesing scheme
to change the range of the values we wanna predict. This move is done in order to get a faster and better convergence
of the machine learning methods, in general.

Which looks like:

![](https://raw.githubusercontent.com/natalidesanti/natalidesanti.github.io/master/images/graph.png)
###### Figure 8: Visualization of one box, with their particles converted into a graph.

In order to get the dataset ready to use it into the GNN we need to convert it into tensors and split it into
training, validation and testing subsets in the right structures. We can do so using the following function:

```
def transform_dataset(dataset, frac_train, frac_val, batch_size):
    '''
    frac_train: fraction of the dataset to use in the training
    frac_val:   fraction of the dataset to use in the validation
    batch_size: number of samples to work through before updating the internal model parameters
    '''

    #Getting the training, validation, and testing sets

    dataset_train = dataset[:int(len(dataset)*frac_train)]
    dataset_val = dataset[int(len(dataset)*frac_train):int(len(dataset)*(frac_train + frac_val))]
    dataset_test = dataset[int(len(dataset)*(frac_train + frac_val)):]

    #Organizing in batches
    train_loader = DataLoader(dataset = dataset_train, batch_size = batch_size)
    val_loader = DataLoader(dataset = dataset_val, batch_size = batch_size)
    test_loader = DataLoader(dataset = dataset_test, batch_size = batch_size)

    return train_loader, val_loader, test_loader
```    

Graph Neural Networks (GNNs)
________________________

Now it is time to tell you something about the most interesting part, the **GNNs**.
Graphs can be used to perform many different tasks. I really love the picture below, which
represents many tasks graphs can be employed to.

![](https://images.datacamp.com/image/upload/v1658404112/Types_of_Graph_Neural_Networks_fd300394e8.png)
###### Figure 9: Graph tasks - Image source: [https://images.datacamp.com/image/upload/v1658404112/Types_of_Graph_Neural_Networks_fd300394e8.png](https://images.datacamp.com/image/upload/v1658404112/Types_of_Graph_Neural_Networks_fd300394e8.png).

In this post I will present to you how to use **graph embeding**. This means we will predic a **global property**
for each graph. We can do so using an amazing archtecture called **message passing scheme** where we are going
to feed the GNN with the graphs and update the values of their nodes and edge attributes according to many 
**layers**/**blocks** without changing the shape of the graphs. This can be done using `Metalayers`
and buiding a model to update the values of the node and edge attributes in the way we desire.

Let's describe the **edge** and **node** models:

* **Edge model:**

$e_{i j}^{(\ell + 1)} = \mathcal{E}^{(\ell + 1)} \left( \left[ n_{i}^{(\ell)}, n_{j}^{(\ell)}, e_{i j}^{(\ell)} \right]  \right)$,

where $\mathcal{E}^{(\ell + 1)}$ represents a MLP;

* **Node model:**

$n_{i}^{(\ell + 1)} = \mathcal{N}^{(\ell + 1)} \left( \left[ n_{i}^{(\ell)}, \bigoplus_{j \in \mathfrak{ N}_i } e \right]  \right)$ <!--- ,  _{i j}^{(\ell + 1)}  , -->

  where $\mathfrak{N}_i$ represents all neighbors of node $i$, $\mathcal{N}^{(\ell + 1)}$ is
  a NN, and $\oplus$ is a multi-pooling operation responsible to concatenate several
  permutation invariant operations:

$\bigoplus_{j \in \mathfrak{N}_i} e_{i j}^{(\ell + 1)} = \left[ \max_{j \in \mathfrak{N}_i} e_{i j}^{(\ell + 1)}, \sum_{j \in \mathfrak{N}_i} e_{i j}^{(\ell + 1)}, \frac{ \sum_{j \in \mathfrak{N}_i} e_{i j}^{(\ell + 1)} }{ \sum_{j \in \mathfrak{N}_i} } \right]$.
  
Several aggregators can enhance the expressiveness of GNN!
